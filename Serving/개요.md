# 용어 정의
* Serving : 모델을 웹/앱 서비스로 배포하는 과정, 모델을 활용하는 방식, 모델을 서비스화하는 관점
* Inference : 모델에 데이터가 제공되어 예측하는 경우, 사용하는 관점
* Serving하고 Inference 혼재되어 사용하는 경우 많음 (둘이 거의 비슷)


# 모델 서빙
* Online Serving : 실시간 serving
* Batch Serving : 일정 주기로 한번에 serving

# Online Serving
#### 웹서버
* HTTP를 통해 웹 브라우저에서 요청하는 HTML 문서나 오브젝트를 전송해주는 서비스 프로그램
* 요청을 받으면 요청한 내용을 보여주는 프로그램

 ![image](https://user-images.githubusercontent.com/63588046/168528795-4d56c15b-650a-4594-b211-badd9bdbff51.png)

#### API
* 운영체제나 프로그래밍 언어가 기능을 제어할 수 있게 만든 인터페이스
* 인터페이스 : 기계와 인간의 소통 창구
* 라이브러리도 API이다.
 
 ## 온라인 serving 구현
 * 직접 API 웹 서버 개발 : Flask, FastAPI  
 * 클라우드 서비스 활용 : AWS의 SageMaker, GCP의 Vertex AI
    * 직접 구축해야 하는 MLOps의 다양한 부분이 만들어짐
    * 사용자 관점에선 PyTorch 사용하듯 학습코드만 제공하면 API 서버 만들어짐
 * Serving 라이브러리 활용 : Torch Serve, MLFlow, BentoML
    * 서버의 이해가 있어야함
    * 다양한 방식으로 개발 가능
    * 추상화된 패턴을 잘 제공하는 오픈소스 활용
    
## 추천 방식
* 프로토타입 모델을 클라우드 서비스 활용해서 배포
* 직접 FastAPI 등을 활용하여 서버 개발
* Serving 라이브러리 활용해 개발

#### 고려할 점
1. Latency : 지연 시간 (짧을수록 좋음)
    * input 데이터를 기반으로 database 있는 데이터 추출해야 하는 경우
        * 데이터는 다양한 공간(DB, AWS)에 저장되어 있을 수 있음
        * 데이터를 추출하기 위해 쿼리 실행, 결과를 받는 시간이 소요
     * 모델 수행 연산
        * RNN, LSTM등 연산량 많음
     
